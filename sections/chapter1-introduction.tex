\chapter{Introduction}

Reactive programming is becoming increasingly popular. Over the past decade, we have observed a slow but steady shift away from traditional imperative paradigms towards other, more declarative ones.
The technology landscape is moving, and with it the software development world. Web and mobile applications, the \textit{internet of things} and now virtual reality software are dominating the conversation, and they all have one thing in common: they are driven by events. Events, callbacks and asynchronous computation have always been challenging. Since callbacks are inherently not composable and do not provide a unified interface, one quickly arrives at deeply nested callbacks to perform any meaningful task, these nested structures are colloquially known as \textit{callback hell}. Furthermore, propagating and eventually surfacing errors requires manual wiring of these errors up the call chain, resulting in repetitive and difficult to maintain code. 

Reactive programming solves these problems by putting events at the forefront as first class citizens under one unified interface, reifying them as time varying values and making them composable using declarative operators. It provides a single interface over I/O, events and any asynchronous computation, switching the mindset from state through imperative modifications to state distilled from declarative derivations of events. 

\section{Problem statement}

Timing and efficiency is critical and desired for reactive systems (e.g. user interfaces, robotics, etc.).
Thus far the majority of reactive programming implementations have been single threaded and don't utilise the maximum potential of their underlying host. Slowly but surely there are parallel approaches popping up which focus on thread based or actor based concurrency \cite{peterson_parallel_2000}, but none have tried mapping these reactive programs onto something that was designed to run parallel from the beginning: the dataflow execution model. This execution model allows instruction invocation whenever the inputs are available (contrary to the sequential models such as the Von Neumann model where instructions are executed one after another) and does not have global state, which allows it to exploit all the available parallelism in a given input program.

Reactive programming nor the dataflow execution model are new technologies, both go back to at least the 1980s \cite{harel_development_1985, veen_dataflow_1986}. While they try to solve different problems, they do share an interesting common trait: the evaluation models of both use directed graphs to orchestrate data flowing through its applications. 

The core principle of reactive programming is making events first class citizens, called signals: they can be listened to, transformed or even composed with other signals, ultimately building a graph of nodes that represent these signals and the data dependencies between them. When the application executes, data courses through this graph via the \textit{reactions} of the nodes when a signal is fired. One could say that the system \textit{reacts} to every signal.

The dataflow execution model on the other hand has a different purpose: parallelizing primitive instructions as much as possible by tracking the flow of data through them and invoking instructions as soon as their inputs are present. Unrelated instructions, which are not dependent on common data, can be executed in parallel, while related instructions are only invoked when their data dependencies are satisfied. These dependencies between instructions again form a graph. This time however, nodes in the graph represent instructions, not signals. Dependencies in this graph simply indicate that one instruction takes as input the output of another. 

So while reactive programming uses a graph of nodes which represent signals, the dataflow execution model has a graph which represents instructions. When executed, data flows through these graphs and the output of a node is forwarded to nodes further down the graph. Conceptually, nodes in these graphs mean different things in the two models, but the way that they behave is undeniably similar.

The goal of this thesis is to explore the possible benefits of running a reactive language on top of a dataflow engine, more specifically the advantages with regards to parallel execution. We propose that the update mechanism necessary to support a reactive language can be completely implemented on top of a dataflow engine, with the aim to parallelize the updating of separate nodes in the reactive graph. 
This would allow us to scale a reactive program horizontally, exploiting the maximum amount of parallelism possible. The purpose of this thesis is therefore to investigate the parallelization of reactive programs by using the dataflow execution model as a platform.

\section{Contributions}

In order to test our hypothesis, we present a new reactive programming language \textit{FrDataFlow}, based on Racket with a few extra features to natively support reactive programming. Expressions in this language are evaluated by our own interpreter, which builds the aforementioned graph in the background and keeps it up to date. Rather than implementing our own update mechanism however, the reactive graph is translated to instructions in a dataflow engine implementation as described in \textit{An Extensible Virtual Machine Design for the Execution of High-level Languages on Tagged-token Dataflow Machines} \cite{saey_extensible_2017}. This is the core mechanism that will orchestrate the events and keep nodes in the graph up to date.

Furthermore, we evaluate this approach by comparing it to an implementation of the same language that does not run on a dataflow engine, but applies a more traditional evaluation model, as described in FrTime \cite{cooper_embedding_2006}. Benchmarks are provided for both latency and throughput for both implementations. Lastly, we also investigate the scaling possibilities of FrDataFlow by actually running it across multiple cores simultaneously. 

\section{Structure}

This dissertation is structured to provide the reader with sufficient context before the actual research and evaluation is discussed. To this end, chapter 2 provides a general background with regard to both reactive programming and the dataflow execution model. It succinctly covers the problems they want to solve, how they attempt to do so, the advantages of these approaches and a short example illustrating how they work.

In chapter 3, our custom language FrDataFlow is presented, with an elaborate sample program displaying most of the features supported by the language. The evaluation of this example by our interpreter is then visually detailed with diagrams and corresponding explanations. 

The dataflow engine is discussed in chapter 4. This is an implementation of the dataflow execution model that powers the update mechanism in FrDataFlow. A short sample is provided to show how instructions are invoked in this engine, and how output from one instruction gets forwarded to the input argument of another. Next, the main research topic of this dissertation is presented: the mapping algorithm. This is the translation we make from the graph in reactive programming to a new graph of instructions for the dataflow engine. Encountered obstacles and corresponding workarounds are discussed.

Chapter 5 details the evaluation of our research. We investigate the consequences of running a reactive system on top of a dataflow engine with regards to efficiency. Furthermore, we discuss the benefits and downsides of running reactive programs in parallel in terms of latency and throughput of values. 

In chapter 6, we summarize the boundaries at which this research has drawn a line and discuss possible future improvements or extensions to our system. We do this by recognizing features provided in other reactive languages or frameworks, that are not present in FrDataFlow. 

Related research topics are discussed in chapter 7. We took great inspiration from ambitious works such as Elm and FrTime, which we compare with FrDataFlow in terms of design and evaluation. 

Finally, we conclude with chapter 8, which summarizes our findings of this research. We revisit the original problem statement and summarize our contributions to this space. 
